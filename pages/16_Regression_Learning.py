"""
Beginner Regression Learning Module
Simple, step-by-step guide for learning regression with visual explanations and real-world examples.
"""
import streamlit as st
import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor

# Optional advanced boosters (with fallback if not installed)
try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
except Exception:
    XGBOOST_AVAILABLE = False
    
try:
    import lightgbm as lgb
    LIGHTGBM_AVAILABLE = True
except Exception:
    LIGHTGBM_AVAILABLE = False
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.inspection import permutation_importance

from src.core.config import load_config
from src.core.ui import app_header, page_navigation, sidebar_dataset_status
from src.core.standardized_ui import (
    standard_section_header,
    beginner_tip,
    before_after_comparison,
    metric_card,
)

st.set_page_config(page_title="Regression Learning", layout="wide", initial_sidebar_state="expanded")

config = load_config()

app_header(
    config,
    page_title="Regression Learning",
    subtitle="Predict numbers and understand model performance",
    icon="ğŸ§‘â€ğŸ“"
)

st.markdown("""
### ğŸ¯ Complete Regression Workflow

**Follow these steps in order for best results:**
1. ğŸ“š **Understand Regression** - Learn the fundamentals
2. ğŸ“‚ **Load Your Data** - Upload or use sample dataset
3. ğŸ¯ **Choose Features & Target** - Select inputs and output
4. ğŸ§ª **Split Data** - Separate training and testing sets
5. âš™ï¸ **Choose Model** - Pick algorithm
6. ğŸš€ **Train & Evaluate** - Build model and check performance
7. ğŸ“Š **Understand Metrics** - Interpret results
8. ğŸ” **Feature Analysis** - See what matters most

**ğŸ“š What is Regression?**

Regression is supervised learning that predicts **continuous numeric values** (not categories).

**Real-world examples:**
- ğŸ  **House Prices**: Predict $320,000 based on size, location, age
- ğŸ“ˆ **Stock Prices**: Forecast tomorrow's price at $152.30
- ğŸŒ¡ï¸ **Temperature**: Predict 72.5Â°F for tomorrow
- ğŸ’° **Sales**: Estimate revenue of $45,230 next month
- â±ï¸ **Delivery Time**: Predict 23.5 minutes

**How it works:**
1. **Training**: Model learns relationship between features (inputs) and target (output)
2. **Prediction**: Given new inputs, model estimates the numeric output
3. **Evaluation**: Measure how close predictions are to actual values

**Key difference from classification:**
- **Regression**: Predicts numbers (23.5, $45,000, 72Â°F)
- **Classification**: Predicts categories (Yes/No, Cat/Dog, High/Low)

**Types of regression problems:**
- **Simple**: One input feature (e.g., predict price from just square footage)
- **Multiple**: Many input features (predict price from size, location, age, bedrooms, etc.)
- **Time series**: Predict future values based on past (stock prices, sales forecasting)
""")

beginner_tip(
    "ğŸ’¡ **New to regression?** Think of it as 'connect the dots' - the model finds the best line/curve "
    "through your data points to make predictions. More data = better line = better predictions!"
)

st.markdown(
    """
    ### ğŸ§­ Fast practice kit
    - **Baseline first:** Fit a simple Linear Regression to set a reference error.
    - **Scale matters:** Standardize numeric features before regularized models (Ridge/Lasso).
    - **Leakage check:** Remove future or target-derived columns (totals that include the target period).
    - **Outlier audit:** Plot histogram/boxplot of the target; winsorize or flag extreme values.
    - **Error lens:** Track both MAE (average miss) and RMSE (penalizes large misses); compare on train vs. test.
    """
)

with st.expander("Interpretation cheat sheet", expanded=False):
    st.markdown(
        """
        - **Sign and size:** For linear models, check coefficient sign and relative magnitude after scaling.
        - **Feature importance:** Use permutation importance to see what actually moves the error needle.
        - **Prediction sanity:** Compare predictions on a few hand-crafted edge cases.
        - **Residual scan:** Plot residuals vs. predictions to spot heteroscedasticity or non-linearity.
        - **Drift watch:** If data is time-based, check whether error changes over time.
        """
    )

sidebar_dataset_status(st.session_state.get("raw_df"), st.session_state.get("clean_df"))

# Step 1: Data Upload or Sample
standard_section_header("Step 1: Choose Your Data", "ğŸ“‚")
st.markdown("""
**Data requirements for regression:**
- âœ… Numeric target column (the value you want to predict)
- âœ… At least one feature column (can be numeric or categorical)
- âœ… Enough rows (50+ minimum, 100+ recommended, 1000+ ideal)
- âœ… No extreme missing values (< 20% missing ideal)

**Sample dataset:**
Simple synthetic data showing linear relationship between a feature and target.
Perfect for learning how regression works!
""")

data_source = st.radio("Select data source:", ["Upload CSV", "Use sample dataset"], horizontal=True)
df = None
if data_source == "Upload CSV":
    uploaded_file = st.file_uploader("Upload your CSV file", type=["csv"])
    if uploaded_file:
        df = pd.read_csv(uploaded_file)
        st.success(f"Loaded {df.shape[0]} rows Ã— {df.shape[1]} columns.")
elif data_source == "Use sample dataset":
    np.random.seed(42)
    X = np.linspace(0, 10, 150)
    y = 2 * X + np.random.normal(0, 2, 150)
    df = pd.DataFrame({'Feature': X, 'Target': y})
    st.success("Loaded sample regression dataset.")

if df is not None:
    st.dataframe(df.head(10), width="stretch")
    st.caption("Preview: First 10 rows of your data.")
    st.markdown("""
    **âœ… Data loaded successfully!**
    
    **What to check:**
    - Do you see your target column (what you want to predict)?
    - Are feature columns present (inputs for prediction)?
    - Any obvious errors or missing values?
    - Do numbers look reasonable (no typos like 99999999)?
    """)
else:
    st.info("Upload a CSV or use the sample dataset to begin.")
    st.stop()

# Step 2: Select Features and Target
standard_section_header("Step 2: Select Features and Target", "ğŸ¯")
st.markdown("""
**ğŸ“š Understanding Features and Target:**

**Target (Output):**
- The number you want to predict
- Examples: price, temperature, sales amount
- Must be a single numeric column
- Should have meaningful variation (not all the same value)

**Features (Inputs):**
- Columns used to make predictions
- Can be numeric (age, income) or categorical (color, region)
- More relevant features = better predictions
- Quality > quantity (10 good features beats 100 random ones)

**Feature selection tips:**
- âœ… Include features you believe influence the target
- âœ… Start with 3-10 features for simplicity
- âŒ Avoid including target-derived columns (data leakage!)
- âŒ Avoid ID columns or random identifiers

**Example:**
- **Target**: House Price
- **Good features**: Square footage, bedrooms, location, age
- **Bad features**: Listing ID, sale date (might leak information)
""")
feature_cols = st.multiselect("Feature columns (inputs):", options=[c for c in df.columns if c != 'Target'], default=[c for c in df.columns if c != 'Target'])
target_col = st.selectbox("Target column (output):", options=[c for c in df.columns if c not in feature_cols])
if not feature_cols or not target_col:
    st.warning("Select at least one feature and a target column.")
    st.stop()

# Properly align X and y - drop rows with NaN in either X or y
df_clean = df[feature_cols + [target_col]].dropna()
if len(df_clean) < 10:
    st.error(f"Not enough valid rows after removing NaN values. You have {len(df_clean)} rows, need at least 10.")
    st.stop()

X = df_clean[feature_cols]
y = df_clean[target_col]

# Step 3: Train/Test Split
standard_section_header("Step 3: Split Data for Training and Testing", "ğŸ§ª")
st.markdown("""
**ğŸ“š Why Split Data?**

Just like students need practice problems AND a final exam:
- **Training set (80%)**: Model learns patterns from this data
- **Test set (20%)**: Evaluates model on new, unseen data

**Why this matters:**
- Testing on training data gives falsely optimistic scores (like memorizing answers)
- Test set simulates real-world performance on future data
- Helps detect overfitting (model memorized training data but can't generalize)

**Rule of thumb:**
- 80/20 split is standard for medium datasets (hundreds to thousands of rows)
- 70/30 for smaller datasets (< 500 rows)
- 90/10 for very large datasets (> 10,000 rows)
""")

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
col1, col2 = st.columns(2)
with col1:
    st.metric("Training Rows", f"{X_train.shape[0]}", help="Model learns from this data")
with col2:
    st.metric("Testing Rows", f"{X_test.shape[0]}", help="Model is evaluated on this unseen data")

# Step 4: Model Selection
standard_section_header("Step 4: Choose a Regression Model", "âš™ï¸")
st.markdown("""
**ğŸ¤– Understanding Regression Models:**

ğŸ“ˆ **Linear Regression** (Simple & Fast)
- **How it works**: Fits the best straight line (y = mx + b) through your data
- **Best for**: When relationship between features and target is roughly linear
- **Strengths**: Fast, interpretable, gives coefficient weights
- **Weaknesses**: Assumes linear relationships, sensitive to outliers
- **Use when**: You need quick results and explainability

ğŸ“ˆ **Ridge Regression** (Regularized Linear)
- **How it works**: Like linear regression but penalizes large coefficients
- **Best for**: When you have many correlated features
- **Strengths**: Prevents overfitting, handles multicollinearity well
- **Weaknesses**: Still assumes linearity
- **Use when**: Linear regression overfits or features are correlated

ğŸ“ˆ **Lasso Regression** (Feature Selection)
- **How it works**: Like Ridge but can shrink coefficients to exactly zero
- **Best for**: When you suspect many features are irrelevant
- **Strengths**: Automatic feature selection, interpretable
- **Weaknesses**: May not perform well with highly correlated features
- **Use when**: You want automatic feature elimination

ğŸ“ˆ **Elastic Net** (Balanced Regularization)
- **How it works**: Mixes Ridge (L2) and Lasso (L1) penalties
- **Best for**: Many correlated features where you still want sparsity
- **Strengths**: Balances feature selection with stability
- **Weaknesses**: Two hyperparameters to tune (alpha, l1_ratio)
- **Use when**: Lasso is too aggressive and Ridge is too loose

ğŸŒ² **Random Forest Regression** (Powerful & Flexible)
- **How it works**: Averages predictions from many decision trees
- **Best for**: Complex non-linear relationships
- **Strengths**: Handles non-linearity, robust to outliers, no scaling needed
- **Weaknesses**: Slower, less interpretable, can overfit with default settings
- **Use when**: Linear models fail or relationships are complex

ğŸŒ„ **Gradient Boosting Regression** (Strong Tabular Default)
- **How it works**: Sequential trees that fix prior errors (boosting)
- **Best for**: Tabular data with non-linearities and interactions
- **Strengths**: Strong accuracy, handles mix of feature types
- **Weaknesses**: More hyperparameters, can overfit without tuning
- **Use when**: You want stronger accuracy than Random Forest with careful tuning

âš¡ **HistGradientBoosting Regression** (Modern, Fast Boosting)
- **How it works**: Histogram-based gradient boosting (like LightGBM style) in scikit-learn
- **Best for**: Medium/large tabular datasets; handles missing values
- **Strengths**: Fast, strong accuracy, supports monotonic constraints (advanced)
- **Weaknesses**: Less interpretable; needs validation to avoid overfitting
- **Use when**: You want a modern, high-performing tree booster without extra deps

ğŸš€ **XGBoost Regression** (Industry Standard Booster)
- **How it works**: Optimized gradient boosting with advanced regularization and parallel trees
- **Best for**: Kaggle competitions, production systems, structured/tabular data
- **Strengths**: State-of-art accuracy, handles missing values, GPU support, built-in CV
- **Weaknesses**: Many hyperparameters to tune; can overfit without careful tuning
- **Use when**: You want top accuracy for structured data and can tune hyperparameters

âš¡ **LightGBM Regression** (Fast & Scalable Booster)
- **How it works**: Leaf-wise gradient boosting (grows best leaf each iteration vs. level-wise)
- **Best for**: Large datasets (>100K rows), fast training with high accuracy
- **Strengths**: Faster than XGBoost, memory-efficient, handles categorical features natively
- **Weaknesses**: Leaf-wise growth can overfit on small data; needs tuning
- **Use when**: You have large datasets and want speed + accuracy; works great with categorical features

**ğŸ¯ Quick Selection Guide:**

| Your Situation | Recommended Model |
|----------------|------------------|
| Simple linear relationship | Linear Regression |
| Many correlated features | Ridge or Elastic Net |
| Want automatic feature selection | Lasso |
| Balanced sparsity + stability | Elastic Net |
| Complex/non-linear patterns | Random Forest |
| Stronger accuracy on tabular | Gradient Boosting / HistGradientBoosting |
| **Top accuracy (competition/production)** | **XGBoost or LightGBM** |
| Large dataset (>100K rows) | LightGBM |
| Need interpretability | Linear, Ridge, Lasso, or Elastic Net |
| Have outliers | Random Forest |
| Have missing values | XGBoost, LightGBM, HistGradientBoosting |
| Categorical features | LightGBM (native support) |

**ğŸ’¡ Pro tip:** Always start with Linear Regression as baseline, then try XGBoost/LightGBM for best accuracy on tabular data. Use regularized models for interpretability.
""")

# Build model list dynamically based on availability
model_options = [
    "Linear Regression",
    "Ridge",
    "Lasso",
    "Elastic Net",
    "Random Forest Regression",
    "Gradient Boosting Regression",
    "HistGradientBoosting Regression",
]

if XGBOOST_AVAILABLE:
    model_options.append("XGBoost Regression")
    
if LIGHTGBM_AVAILABLE:
    model_options.append("LightGBM Regression")

# Show installation notice if advanced models aren't available
if not XGBOOST_AVAILABLE or not LIGHTGBM_AVAILABLE:
    missing = []
    if not XGBOOST_AVAILABLE:
        missing.append("XGBoost")
    if not LIGHTGBM_AVAILABLE:
        missing.append("LightGBM")
    
    with st.expander(f"âš ï¸ Optional models not available: {', '.join(missing)}", expanded=False):
        st.markdown(f"""
        **{', '.join(missing)} could not be loaded.** These are optional high-performance models.
        
        **To enable them:**
        """)
        if not XGBOOST_AVAILABLE:
            st.code("# For macOS users - install OpenMP first\nbrew install libomp\n\n# Then reinstall xgboost\npip install --upgrade --force-reinstall xgboost", language="bash")
        if not LIGHTGBM_AVAILABLE:
            st.code("pip install --upgrade lightgbm", language="bash")
        st.info("The app works fine without these models. You can use other algorithms like Random Forest or HistGradientBoosting.")

model_name = st.selectbox("Model:", model_options)

if model_name == "Linear Regression":
    model = LinearRegression()
    st.info("âœ… Simple and interpretable. Best for linear relationships.")
elif model_name == "Ridge":
    model = Ridge()
    st.info("âœ… Handles correlated features well. Good for preventing overfitting.")
elif model_name == "Lasso":
    model = Lasso()
    st.info("âœ… Automatic feature selection. Sets unimportant features to zero.")
elif model_name == "Elastic Net":
    model = ElasticNet()
    st.info("âœ… Blends Ridge + Lasso. Good when you want sparsity without instability.")
elif model_name == "Random Forest Regression":
    model = RandomForestRegressor(random_state=42)
    st.info("âœ… Powerful for complex patterns. Handles non-linearity naturally.")
elif model_name == "Gradient Boosting Regression":
    model = GradientBoostingRegressor(random_state=42)
    st.info("âœ… Strong tabular baseline with boosting. Good for non-linear interactions.")
elif model_name == "HistGradientBoosting Regression":
    model = HistGradientBoostingRegressor(random_state=42)
    st.info("âœ… Modern, fast booster (sklearn's LightGBM-style). Handles missing values.")
elif model_name == "XGBoost Regression":
    if XGBOOST_AVAILABLE:
        model = xgb.XGBRegressor(random_state=42, n_estimators=100, learning_rate=0.1)
        st.info("âœ… ğŸ† Industry-standard booster. Top accuracy for structured data with proper tuning.")
    else:
        st.error("XGBoost is not available. See installation instructions above.")
        st.stop()
elif model_name == "LightGBM Regression":
    if LIGHTGBM_AVAILABLE:
        model = lgb.LGBMRegressor(random_state=42, n_estimators=100, learning_rate=0.1, verbose=-1)
        st.info("âœ… ğŸš€ Fast and scalable. Excellent for large datasets and categorical features.")
    else:
        st.error("LightGBM is not available. See installation instructions above.")
        st.stop()
else:
    st.error("Unknown model selected.")
    st.stop()

# Step 5: Train the Model
standard_section_header("Step 5: Train Your Model & Evaluate Results", "ğŸš€")
if st.button("Train Model", type="primary"):
    with st.spinner("Training model... â³"):
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        mae = mean_absolute_error(y_test, y_pred)
        mse = mean_squared_error(y_test, y_pred)
        rmse = np.sqrt(mse)
        r2 = r2_score(y_test, y_pred)
    
    st.success(f"âœ… Model trained successfully!")
    
    st.markdown("""
    ---
    ### ğŸ“Š Understanding Regression Metrics
    
    Different metrics tell you different things about prediction quality:
    
    ğŸ¯ **MAE (Mean Absolute Error)**
    - **Formula**: Average of |actual - predicted|
    - **What it means**: Average error size in original units
    - **Example**: MAE of $5,000 means predictions are off by $5k on average
    - **When to use**: When all errors matter equally, easy to explain
    - **Good value**: Depends on scale; compare to target's standard deviation
    - **Pros**: Easy to understand, robust to outliers
    - **Cons**: Doesn't heavily penalize large errors
    
    ğŸ¯ **MSE (Mean Squared Error)**
    - **Formula**: Average of (actual - predicted)Â²
    - **What it means**: Average squared error (penalizes large mistakes more)
    - **Why squared**: Amplifies large errors (error of 10 counts as 100)
    - **When to use**: When large errors are particularly bad
    - **Challenge**: Units are squared (hard to interpret directly)
    - **Pros**: Differentiable (good for optimization), penalizes outliers
    - **Cons**: Sensitive to outliers, units are squared
    
    ğŸ¯ **RMSE (Root Mean Squared Error)**
    - **Formula**: âˆšMSE
    - **What it means**: Like MSE but back in original units
    - **Example**: RMSE of $7,000 means typical error is $7k (more weight on large errors than MAE)
    - **When to use**: When you want to penalize large errors but keep interpretable units
    - **Interpretation**: Usually larger than MAE (due to squaring large errors)
    - **Pros**: Same units as target, penalizes large errors
    - **Cons**: Still sensitive to outliers
    
    ğŸ¯ **RÂ² Score (Coefficient of Determination)**
    - **Formula**: 1 - (Sum of Squared Errors / Total Variance)
    - **Range**: -âˆ to 1.0 (usually 0 to 1)
    - **What it means**: % of variance in target explained by model
    - **Interpretation**:
      - **1.0** = Perfect predictions âœ…
      - **0.8-0.9** = Excellent predictions
      - **0.7-0.8** = Good predictions
      - **0.5-0.7** = Moderate predictions
      - **< 0.5** = Poor predictions
      - **0.0** = Model as good as predicting the mean
      - **< 0.0** = Model worse than predicting the mean! âš ï¸
    - **When to use**: Comparing models on same data, scale-independent metric
    - **Pros**: Easy to interpret (0-1 scale), scale-independent
    - **Cons**: Can be misleading with non-linear relationships
    
    **ğŸ“ˆ Visual interpretation aids:**
    - **Actual vs Predicted plot**: Points should align with diagonal line
    - **Error distribution**: Should be centered around zero with symmetrical spread
    - **Residuals plot**: Random scatter = good; patterns = model missing something
    
    **ğŸ¯ Which metric to prioritize?**
    
    | Scenario | Best Metric | Why |
    |----------|------------|-----|
    | Easy interpretation | MAE or RÂ² | Simple to explain to stakeholders |
    | Outliers present | MAE | Less sensitive to extreme values |
    | Large errors very bad | RMSE or MSE | Penalizes big mistakes heavily |
    | Comparing models | RÂ² | Scale-independent comparison |
    | General purpose | RMSE + RÂ² | RMSE for magnitude, RÂ² for fit quality |
    
    **ğŸ’¡ Rules of thumb:**
    - MAE < 10% of target range = Good model
    - RÂ² > 0.7 = Good explanatory power
    - RMSE should be compared to target's std deviation (RMSE < 0.5Ã—std is good)
    - If RÂ² < 0: Model is broken or data has issues!
    """)
    
    m1, m2, m3, m4 = st.columns(4)
    with m1:
        st.metric("MAE", f"{mae:.3f}", help="Average absolute error")
        st.caption(f"Lower is better")
    with m2:
        st.metric("RMSE", f"{rmse:.3f}", help="Root mean squared error")
        st.caption(f"Penalizes large errors")
    with m3:
        st.metric("RÂ² Score", f"{r2:.3f}", help="Variance explained (0-1)")
        if r2 >= 0.8:
            st.caption("âœ… Excellent!")
        elif r2 >= 0.7:
            st.caption("âœ… Good")
        elif r2 >= 0.5:
            st.caption("âš ï¸ Moderate")
        else:
            st.caption("âŒ Needs improvement")
    with m4:
        # Calculate MAPE if no zeros in actual values
        if (y_test == 0).any():
            st.metric("MAPE", "N/A", help="Not available (zero values present)")
        else:
            mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
            st.metric("MAPE", f"{mape:.1f}%", help="Mean Absolute Percentage Error")
            st.caption(f"Avg error %")

    st.markdown("---")
    st.markdown("#### ğŸ“Š Visual Results Analysis")
    
    # Actual vs Predicted
    col_vis1, col_vis2 = st.columns(2)
    
    # Actual vs Predicted
    col_vis1, col_vis2 = st.columns(2)
    
    with col_vis1:
        st.markdown("**Actual vs Predicted**")
        st.markdown("Points should cluster near the diagonal line for good predictions")
        actual_pred_df = pd.DataFrame({
            'Actual': y_test.values,
            'Predicted': y_pred,
            'Index': range(len(y_test))
        })
        
        # Use Plotly for better visualization
        import plotly.graph_objects as go
        fig = go.Figure()
        
        # Scatter plot
        fig.add_trace(go.Scatter(
            x=y_test.values,
            y=y_pred,
            mode='markers',
            name='Predictions',
            marker=dict(size=8, opacity=0.6, color='blue')
        ))
        
        # Perfect prediction line
        min_val = min(y_test.min(), y_pred.min())
        max_val = max(y_test.max(), y_pred.max())
        fig.add_trace(go.Scatter(
            x=[min_val, max_val],
            y=[min_val, max_val],
            mode='lines',
            name='Perfect Prediction',
            line=dict(color='red', dash='dash', width=2)
        ))
        
        fig.update_layout(
            xaxis_title="Actual Values",
            yaxis_title="Predicted Values",
            height=350,
            showlegend=True
        )
        st.plotly_chart(fig, width="stretch")
        st.caption("âœ… Good: Points near red line | âŒ Bad: Points scattered far from line")
    
    with col_vis2:
        st.markdown("**Error Distribution (Residuals)**")
        st.markdown("Should be centered around zero with symmetric spread")
        errors = y_test.values - y_pred
        
        fig_err = go.Figure()
        fig_err.add_trace(go.Histogram(
            x=errors,
            nbinsx=30,
            name='Errors',
            marker=dict(color='steelblue')
        ))
        
        fig_err.update_layout(
            xaxis_title="Prediction Error",
            yaxis_title="Frequency",
            height=350,
            showlegend=False
        )
        st.plotly_chart(fig_err, width="stretch")
        st.caption("âœ… Good: Bell curve centered at 0 | âŒ Bad: Skewed or multiple peaks")

    st.markdown("""
    ---
    ### ğŸ’¡ Interpreting Your Results
    
    **If RÂ² is high (> 0.7):**
    - âœ… Model captures patterns well
    - âœ… Features are predictive
    - Next: Deploy or try advanced models for small improvements
    
    **If RÂ² is moderate (0.5-0.7):**
    - âš ï¸ Model explains some variance but missing patterns
    - Try: Add more relevant features, engineer interactions, try non-linear models
    
    **If RÂ² is low (< 0.5):**
    - âŒ Model doesn't capture relationship well
    - Check: Are features actually related to target? Is there enough data?
    - Try: Collect more data, add better features, check for data quality issues
    
    **If RÂ² is negative:**
    - ğŸš¨ Model is worse than predicting the mean!
    - Problem: Usually indicates serious issues (wrong model, data leakage, bugs)
    - Action: Check data, revisit feature selection, try simpler model
    
    **Error pattern analysis:**
    - **Random scatter in residuals**: Good! âœ…
    - **Funnel shape** (errors increase with value): Consider log transformation
    - **Patterns or curves**: Model missing non-linear relationships
    - **Outliers**: Consider removing or using robust methods
    """)

    st.markdown("---")
    # Explainability: Permutation Importance
    try:
        standard_section_header("Step 6: Feature Importance Analysis", "ğŸ§ª")
        st.markdown("""
        **ğŸ“š Understanding Feature Importance:**
        
        **What is it?**
        - Shows which features most influence predictions
        - Helps identify key drivers and redundant features
        
        **How it's calculated (Permutation Importance):**
        1. Measure baseline model performance
        2. Randomly shuffle one feature's values
        3. Measure new performance
        4. Importance = drop in performance
        5. Repeat for all features
        
        **How to interpret:**
        - **High importance**: Feature strongly influences predictions (keep!)
        - **Near-zero**: Feature doesn't help much (consider removing)
        - **Negative**: Rare, but means shuffling improved model (red flag - may indicate overfitting)
        
        **What to do with results:**
        - Focus engineering efforts on high-importance features
        - Remove low-importance features to simplify model
        - Investigate unexpected importances (domain validation)
        - Look for feature groups (related features may share importance)
        
        **ğŸ’¡ Pro tips:**
        - For Linear/Ridge/Lasso: Can also check coefficients directly
        - Random Forest: Also has built-in importance (faster but less reliable)
        - Multiple runs give confidence intervals (importance Â± std deviation)
        """)
        
        result = permutation_importance(model, X_test, y_test, scoring="r2", n_repeats=5, random_state=42)
        importances = pd.DataFrame({
            "Feature": X.columns,
            "Importance": result.importances_mean,
            "Std Dev": result.importances_std,
        }).sort_values("Importance", ascending=False)
        
        # Visualize importance
        fig_imp = go.Figure()
        fig_imp.add_trace(go.Bar(
            x=importances["Importance"],
            y=importances["Feature"],
            orientation='h',
            error_x=dict(type='data', array=importances["Std Dev"]),
            marker=dict(color='skyblue')
        ))
        fig_imp.update_layout(
            title="Feature Importance (with std deviation)",
            xaxis_title="Importance Score",
            yaxis_title="Feature",
            height=max(300, len(importances) * 30)
        )
        st.plotly_chart(fig_imp, width="stretch")
        
        st.dataframe(importances, width="stretch")
        
        # Interpretation help
        top_feature = importances.iloc[0]["Feature"]
        top_importance = importances.iloc[0]["Importance"]
        st.success(f"ğŸ¯ **Key Finding**: '{top_feature}' is the most important feature (importance: {top_importance:.4f})")
        
        low_importance_features = importances[importances["Importance"] < 0.01]["Feature"].tolist()
        if low_importance_features:
            st.info(f"ğŸ’¡ **Optimization opportunity**: Consider removing low-importance features: {', '.join(low_importance_features)}")
        
    except Exception as e:
        st.caption(f"Feature importance unavailable: {e}")

    st.markdown("---")
    # Advanced Explainability: SHAP (if installed)
    if st.session_state.get("ui_mode") == "Advanced":
        try:
            import shap
            standard_section_header("Advanced Explainability (SHAP)", "ğŸ”")
            st.markdown("""
            **SHAP (SHapley Additive exPlanations):**
            - More sophisticated than permutation importance
            - Shows both magnitude AND direction of impact
            - Can explain individual predictions
            - Based on game theory (fair credit allocation)
            """)
            
            sample_X = X_test.copy()
            if len(sample_X) > 200:
                sample_X = sample_X.sample(200, random_state=42)
            explainer = shap.Explainer(model, X_train)
            explanation = explainer(sample_X)
            vals = explanation.values
            mean_abs = np.mean(np.abs(vals), axis=0)
            shap_df = pd.DataFrame({
                "Feature": X.columns,
                "Mean |SHAP|": mean_abs,
            }).sort_values("Mean |SHAP|", ascending=False)
            st.dataframe(shap_df.head(10), width="stretch")
            st.caption("Mean absolute SHAP values indicate average feature impact on predictions. Requires 'shap' package.")
        except Exception as e:
            st.caption(f"SHAP explainability unavailable: {e}. Tip: Install with 'pip install shap'")
else:
    st.info("ğŸ‘† Click 'Train Model' above to see results, visualizations, and feature importance analysis.")

page_navigation("6")
